# ğŸ ABC Optimizer Lib: Canonical Artificial Bee Colony Algorithm

![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Scikit-Learn](https://img.shields.io/badge/sklearn-compatible-orange)

**ABC Optimizer Lib**, DerviÅŸ KaraboÄŸa (2005) tarafÄ±ndan Ã¶nerilen **Yapay ArÄ± Kolonisi (Artificial Bee Colony)** algoritmasÄ±nÄ±n %100 kanonik (standart) implementasyonunu iÃ§eren bir Python kÃ¼tÃ¼phanesidir.

Bu kÃ¼tÃ¼phane iki temel amaÃ§ iÃ§in geliÅŸtirilmiÅŸtir:
1.  **Matematiksel Optimizasyon:** Herhangi bir fonksiyonun minimum noktasÄ±nÄ± bulmak.
2.  **Hyperparameter Tuning:** Scikit-Learn uyumlu (LightGBM, XGBoost, vb.) modellerin hiperparametrelerini optimize etmek.

---

## ğŸš€ Ã–zellikler

* **Scikit-Learn Wrapper:** `GridSearchCV` mantÄ±ÄŸÄ±yla Ã§alÄ±ÅŸÄ±r. `fit()` ve `predict()` metodlarÄ±nÄ± destekler.
* **Kanonik Algoritma:** LiteratÃ¼rdeki orijinal Ä°ÅŸÃ§i, GÃ¶zcÃ¼ (Rulet TekerleÄŸi) ve KaÅŸif arÄ± fazlarÄ±na sadÄ±k kalÄ±nmÄ±ÅŸtÄ±r.
* **Hafif ve HÄ±zlÄ±:** Sadece `numpy` ve `scikit-learn` baÄŸÄ±mlÄ±lÄ±ÄŸÄ± vardÄ±r.
* **Esnek:** Hem sÃ¼rekli (float) hem ayrÄ±k (int/categorical) parametre uzaylarÄ±nÄ± destekler.

---

## ğŸ“¦ Kurulum

Bu kÃ¼tÃ¼phaneyi doÄŸrudan GitHub Ã¼zerinden `pip` ile kurabilirsiniz:

```bash
pip install git+[https://github.com/yusufkorkmazyigit/abc-optimizer-lib.git](https://github.com/yusufkorkmazyigit/abc-optimizer-lib.git)
```
GeliÅŸtirme yapmak (kodu deÄŸiÅŸtirmek) isterseniz:

```
git clone [https://github.com/yusufkorkmazyigit/abc-optimizer-lib.git](https://github.com/yusufkorkmazyigit/abc-optimizer-lib.git)
cd abc-optimizer-lib
pip install -e .
```
## ğŸ“– KullanÄ±m Ã–rnekleri
1. LightGBM Hiperparametre Optimizasyonu
Makine Ã¶ÄŸrenmesi modellerinizde en iyi parametreleri bulmak iÃ§in `ABCSearchCV` sÄ±nÄ±fÄ±nÄ± kullanÄ±n:

```
import lightgbm as lgb
from sklearn.datasets import load_breast_cancer
from abc_algorithm import ABCSearchCV

# Veri ve Model
data = load_breast_cancer()
X, y = data.data, data.target
model = lgb.LGBMClassifier(verbosity=-1)

# Arama UzayÄ±
param_space = {
    'learning_rate': {'type': 'float', 'range': (0.01, 0.3)},
    'n_estimators':  {'type': 'int',   'range': (50, 500)},
    'num_leaves':    {'type': 'int',   'range': (20, 100)}
}

# Optimizasyon
abc = ABCSearchCV(
    estimator=model,
    param_space=param_space,
    cv=3,
    scoring='accuracy',
    pop_size=20,    # Koloni boyutu
    max_evals=100   # Toplam deneme sayÄ±sÄ±
)

abc.fit(X, y)

print("En iyi skor:", abc.best_score_)
print("En iyi parametreler:", abc.best_params_)
```
2. Matematiksel Fonksiyon Minimizasyonu
Sadece bir denklemi Ã§Ã¶zmek isterseniz `CanonicalABCSolver` kullanÄ±n:

```
from abc_algorithm import CanonicalABCSolver

# Hedef: Sphere Fonksiyonu (x^2 toplamÄ± 0 olmalÄ±)
def objective(x):
    return sum(x**2)

solver = CanonicalABCSolver(
    objective_func=objective,
    n_params=3,
    lb=[-10, -10, -10],
    ub=[10, 10, 10],
    max_evals=500
)

best_params, best_cost, _ = solver.solve()
print(f"SonuÃ§: {best_params}, Maliyet: {best_cost:.5f}")
```
## ğŸ§  Algoritma MantÄ±ÄŸÄ±
ABC algoritmasÄ±, doÄŸadaki arÄ±larÄ±n yiyecek arama davranÄ±ÅŸlarÄ±nÄ± taklit eder ve Ã¼Ã§ fazdan oluÅŸur:

**Ä°ÅŸÃ§i ArÄ±lar (Employed Bees):** Mevcut bir kaynaÄŸÄ± (Ã§Ã¶zÃ¼mÃ¼) komÅŸuluk araÅŸtÄ±rmasÄ± ile geliÅŸtirmeye Ã§alÄ±ÅŸÄ±r.

**GÃ¶zcÃ¼ ArÄ±lar (Onlooker Bees):** Ä°ÅŸÃ§i arÄ±larÄ±n getirdiÄŸi nektar bilgisine (fitness) gÃ¶re Rulet TekerleÄŸi yÃ¶ntemiyle seÃ§im yapar. Ä°yi kaynaklar daha Ã§ok araÅŸtÄ±rÄ±lÄ±r.

**KaÅŸif ArÄ±lar (Scout Bees):** Belirli bir sÃ¼re geliÅŸtirilemeyen (`limit`) kaynaklar terk edilir ve rastgele yeni bir Ã§Ã¶zÃ¼m aranÄ±r.

---

## ğŸ”¬ GerÃ§ek Hayat UygulamasÄ±: Federated Learning Optimizasyonu

Bu kÃ¼tÃ¼phane kullanÄ±larak, **MedMNIST** veriseti Ã¼zerinde **Non-IID (Dengesiz) Veri** daÄŸÄ±lÄ±mÄ±na sahip bir **Federated Learning** mimarisi optimize edilmiÅŸtir.

**Senaryo:**
* **Veri Seti:** PathMNIST (BaÄŸÄ±rsak dokusu sÄ±nÄ±flandÄ±rma).
* **Problem:** 5 farklÄ± hastaneye (istemciye) dengesiz daÄŸÄ±tÄ±lmÄ±ÅŸ veri. Standart `FedAvg` algoritmasÄ± bu durumda zorlanmaktadÄ±r.
* **Ã‡Ã¶zÃ¼m:** `CanonicalABCSolver` kullanÄ±larak Learning Rate ve Momentum parametreleri optimize edilmiÅŸtir.

**SonuÃ§lar:**
ABC ile optimize edilmiÅŸ model, standart parametrelere gÃ¶re daha hÄ±zlÄ± yakÄ±nsamÄ±ÅŸ ve **%7 daha yÃ¼ksek doÄŸruluk** elde etmiÅŸtir.

![ABC vs Standard FedAvg](./examples/abc_fedavg_final_result.png)

ğŸ”— **[TÃ¼m kodu ve detaylÄ± analizi incelemek iÃ§in tÄ±klayÄ±n](./examples/Federated_Learning_MedMNIST_Optimization.ipynb)**

---

## ğŸ“š Referans
Karaboga, D. (2005). An idea based on honey bee swarm for numerical optimization. Technical report-tr06, Erciyes University, engineering faculty, computer engineering department.

## ğŸ“ Lisans
Bu proje MIT LisansÄ± ile sunulmuÅŸtur.
